{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    Context,\n",
    "    step\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class VerifyCodeEvent(Event):\n",
    "    code_sample : str\n",
    "\n",
    "class CodeAnalysisEvent(Event):\n",
    "    code_analysis : dict\n",
    "\n",
    "class SearchInWebEvent(Event):\n",
    "    pass\n",
    "\n",
    "class ValidateResultEvent(Event):\n",
    "    pass\n",
    "\n",
    "class NotFoundEvent(Event):\n",
    "    pass\n",
    "\n",
    "class UpdateDBEvent(Event):\n",
    "    pass\n",
    "\n",
    "class SearchEventResult(Event):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "import json, os\n",
    "\n",
    "class InfoFusionWorkflow(Workflow):\n",
    "\n",
    "    @step(pass_context = True)\n",
    "    async def take_user_input(self, ctx : Context, ev : StartEvent | CodeAnalysisEvent) -> VerifyCodeEvent | StopEvent:\n",
    "        if isinstance(ev, StartEvent):\n",
    "            if hasattr(ev, 'query'):\n",
    "                await ctx.set('query', ev.query)\n",
    "\n",
    "            if hasattr(ev, 'llm'):\n",
    "                await ctx.set('llm', ev.llm)\n",
    "\n",
    "            if hasattr(ev, 'tools'):\n",
    "                await ctx.set('tools', ev.tools)\n",
    "\n",
    "            human_prompt = \"\"\"\n",
    "                Enter your code for verification or press q/Q to quit.\n",
    "    \"\"\"\n",
    "        else:\n",
    "            if 'response' in ev.code_analysis:\n",
    "                human_prompt = str(ev.code_analysis)\n",
    "            else: \n",
    "                human_prompt = \"\"\"\n",
    "                Here is the suggestion for your code.\n",
    "                \n",
    "                {str(ev.code_analysis)}\n",
    "\n",
    "                Modify the code as per the suggessions and submit the code. If you want to quit press q/Q.\n",
    "\"\"\"\n",
    "\n",
    "        human_input = input (human_prompt)\n",
    "        print(f\"HI:{human_input}\")\n",
    "\n",
    "        quit_str = \"q\"\n",
    "        human_input = human_input.strip()\n",
    "        print(f\"HI:*{human_input}*, *{quit_str}*\")\n",
    "        if human_input.lower() is quit_str:\n",
    "            print(\"sending stop\")\n",
    "            ctx.send_event(StopEvent(\"User decided to stop\"))\n",
    "\n",
    "        ctx.send_event(VerifyCodeEvent(code_sample = human_input))\n",
    "\n",
    "   \n",
    "    @step(pass_context = True)\n",
    "    async def verify_code(self, ctx : Context, ev : VerifyCodeEvent) -> CodeAnalysisEvent: #QueryEvent:\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Given a code sample verify that the code is correct. Do not provide an answer directly, instead identify the mistakes. \n",
    "Provide some reference articals so that the user can fix the error by themselve. Perform a web-search, donot makeup any information.\n",
    "\n",
    "Respond in pure JSON without any markdown, like this :\n",
    "{{\n",
    "    \"Code_Analysis\" : {{\n",
    "        \"code_sample\" : \"\",\n",
    "        \"code_language\" : \"\",\n",
    "        \"code_mistakes\" : [\n",
    "        {{\"mistake1\", \"reference1\"}},\n",
    "        {{\"mistake2\", \"reference2\"}},\n",
    "        ]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "if there are no mistakes in the code, respond in pure JSON without any markdown, like this:\n",
    "{{\n",
    "    \"Code_Analysis\" : {{\n",
    "        \"code_sample\" : \"\",\n",
    "        \"code_language\" : \"\",\n",
    "        \"response\" : \"Congratulations on successfully writing a correct code.\"\n",
    "        }}\n",
    "}}\n",
    "\n",
    "Here is the user code : {ev.code_sample}\n",
    "\"\"\"\n",
    "        agent = ReActAgent.from_tools(tools = [], llm = await ctx.get('llm'), verbose = True)\n",
    "        response = agent.chat(prompt)\n",
    "        # llm = await ctx.get('llm')\n",
    "        # response = llm.complete(prompt)\n",
    "        print(f\"\\nResponse: {response}\")\n",
    "\n",
    "        response_json = json.loads(str(response))\n",
    "\n",
    "        print(f\"json:\\n{response_json}\\n\")\n",
    "\n",
    "        \n",
    "        ctx.send_event(CodeAnalysisEvent(code_analysis = response_json))\n",
    "\n",
    "    # @step(pass_context = True)\n",
    "    # async def query_router(self, ctx : Context, ev : QueryEvent) -> StopEvent: #SearchInWebEvent: #| SearchInDBEvent :\n",
    "    #     pass\n",
    "        \n",
    "\n",
    "    # @step(pass_context = True)\n",
    "    # async def search_in_db(self, ev : SearchInDBEvent) -> ValidateResultEvent | NotFoundEvent:\n",
    "    #     pass\n",
    "\n",
    "    # @step(pass_context = True)\n",
    "    # async def search_in_web(self, ev : SearchInWebEvent) -> ValidateResultEvent:\n",
    "    #     pass\n",
    "\n",
    "    # @step(pass_context = True)\n",
    "    # async def update_db(self, ev : UpdateDBEvent) -> StopEvent:\n",
    "    #     pass\n",
    "\n",
    "    # @step(pass_context = True)\n",
    "    # async def generate_final_result(self, ev : SearchEventResult) -> StopEvent | None:\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step take_user_input\n",
      "HI:q\n",
      "HI:*q*, *q*\n",
      "Step take_user_input produced no event\n",
      "Running step verify_code\n",
      "> Running step 1fca57cf-0625-4f24-8d5c-403558f2d71f. Step input: \n",
      "Given a code sample verify that the code is correct. Do not provide an answer directly, instead identify the mistakes. \n",
      "Provide some reference articals so that the user can fix the error by themselve. Perform a web-search, donot makeup any information.\n",
      "\n",
      "Respond in pure JSON without any markdown, like this :\n",
      "{\n",
      "    \"Code_Analysis\" : {\n",
      "        \"code_sample\" : \"\",\n",
      "        \"code_language\" : \"\",\n",
      "        \"code_mistakes\" : [\n",
      "        {\"mistake1\", \"reference1\"},\n",
      "        {\"mistake2\", \"reference2\"},\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "if there are no mistakes in the code, respond in pure JSON without any markdown, like this:\n",
      "{\n",
      "    \"Code_Analysis\" : {\n",
      "        \"code_sample\" : \"\",\n",
      "        \"code_language\" : \"\",\n",
      "        \"response\" : \"Congratulations on successfully writing a correct code.\"\n",
      "        }\n",
      "}\n",
      "\n",
      "Here is the user code : q\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: {\n",
      "    \"Code_Analysis\" : {\n",
      "        \"code_sample\" : \"q\",\n",
      "        \"code_language\" : \"unknown\",\n",
      "        \"response\" : \"Congratulations on successfully writing a correct code.\"\n",
      "    }\n",
      "}\n",
      "\u001b[0m\n",
      "Response: {\n",
      "    \"Code_Analysis\" : {\n",
      "        \"code_sample\" : \"q\",\n",
      "        \"code_language\" : \"unknown\",\n",
      "        \"response\" : \"Congratulations on successfully writing a correct code.\"\n",
      "    }\n",
      "}\n",
      "json:\n",
      "{'Code_Analysis': {'code_sample': 'q', 'code_language': 'unknown', 'response': 'Congratulations on successfully writing a correct code.'}}\n",
      "\n",
      "Step verify_code produced no event\n",
      "Running step take_user_input\n",
      "HI:q\n",
      "HI:*q*, *q*\n",
      "Step take_user_input produced no event\n",
      "Running step verify_code\n",
      "> Running step f0ec84e6-2816-4a48-a0c0-342f98d8afd8. Step input: \n",
      "Given a code sample verify that the code is correct. Do not provide an answer directly, instead identify the mistakes. \n",
      "Provide some reference articals so that the user can fix the error by themselve. Perform a web-search, donot makeup any information.\n",
      "\n",
      "Respond in pure JSON without any markdown, like this :\n",
      "{\n",
      "    \"Code_Analysis\" : {\n",
      "        \"code_sample\" : \"\",\n",
      "        \"code_language\" : \"\",\n",
      "        \"code_mistakes\" : [\n",
      "        {\"mistake1\", \"reference1\"},\n",
      "        {\"mistake2\", \"reference2\"},\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "if there are no mistakes in the code, respond in pure JSON without any markdown, like this:\n",
      "{\n",
      "    \"Code_Analysis\" : {\n",
      "        \"code_sample\" : \"\",\n",
      "        \"code_language\" : \"\",\n",
      "        \"response\" : \"Congratulations on successfully writing a correct code.\"\n",
      "        }\n",
      "}\n",
      "\n",
      "Here is the user code : q\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: {\n",
      "    \"Code_Analysis\" : {\n",
      "        \"code_sample\" : \"q\",\n",
      "        \"code_language\" : \"unknown\",\n",
      "        \"response\" : \"Congratulations on successfully writing a correct code.\"\n",
      "    }\n",
      "}\n",
      "\u001b[0m\n",
      "Response: {\n",
      "    \"Code_Analysis\" : {\n",
      "        \"code_sample\" : \"q\",\n",
      "        \"code_language\" : \"unknown\",\n",
      "        \"response\" : \"Congratulations on successfully writing a correct code.\"\n",
      "    }\n",
      "}\n",
      "json:\n",
      "{'Code_Analysis': {'code_sample': 'q', 'code_language': 'unknown', 'response': 'Congratulations on successfully writing a correct code.'}}\n",
      "\n",
      "Step verify_code produced no event\n",
      "Running step take_user_input\n"
     ]
    }
   ],
   "source": [
    "import os, traceback\n",
    "\n",
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "\n",
    "tavily_tool = TavilyToolSpec(\n",
    "    api_key=os.environ['TAVILY_API_KEY'],\n",
    ")\n",
    "\n",
    "\n",
    "llm = OpenAI(model = \"gpt-4o-mini\", temperature = 0)\n",
    "workflow = InfoFusionWorkflow(timeout = 60, verbose = True)\n",
    "\n",
    "tools = tavily_tool.to_tool_list()\n",
    "code_sample = \"\"\"\n",
    "int main() {\n",
    "    i = 20\n",
    "    print(i)\n",
    "\n",
    "    return 0\n",
    "}\n",
    "\"\"\"\n",
    "try:\n",
    "    result = await workflow.run(llm = llm,\n",
    "                                tools = tools,\n",
    "                                code_sample = code_sample)\n",
    "\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Exception: {e} \\n {traceback.format_exc()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowAdapter:\n",
    "    def __init__(self):\n",
    "        import os, traceback\n",
    "\n",
    "        from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "\n",
    "        tavily_tool = TavilyToolSpec(\n",
    "            api_key=os.environ['TAVILY_API_KEY'],\n",
    "        )\n",
    "\n",
    "\n",
    "        self.llm = OpenAI(model = \"gpt-4o-mini\", temperature = 0)\n",
    "        self.tools = tavily_tool.to_tool_list()\n",
    "\n",
    "    async def verify_code(self, code_sample):\n",
    "        workflow = InfoFusionWorkflow(timeout = 60, verbose = True)\n",
    "\n",
    "        result = None\n",
    "        try:\n",
    "            result = await workflow.run(llm = self.llm,\n",
    "                                        tools = self.tools,\n",
    "                                        code_sample = code_sample)\n",
    "\n",
    "            print(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e} \\n {traceback.format_exc()}\")\n",
    "            result = \"Exception : {e}\"\n",
    "            \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (infofusionenv)",
   "language": "python",
   "name": "infofusionenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
